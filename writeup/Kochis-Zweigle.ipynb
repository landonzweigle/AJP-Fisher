{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Reinforcement Learning to Play a Fishing Game\n",
    "*By Keegan Kochis and Landon Zweigle*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "Ever since we have gotten into programming we have had a desire to do reinforcement learning, and this term project seemed like a perfect excuse to finally do it. When it came to deciding what to do RL on, we decided to use a [JavaFX](https://openjfx.io/) game Landon made a few years ago; a very simple fishing minigame where the goal is to put a bobber over a fish for a certain amount of time. This game had everything we were looking for; an evironment which is rather simple, but not too simple; and an easy way to communicate with it.\n",
    "\n",
    "\n",
    "\n",
    "The game as described earlier is a simple fishing minigame. It is actually a clone of minigame in a very popular indie game [Stardew Valley](https://www.stardewvalley.net/). Here is an indepth [description](https://stardewvalleywiki.com/Fishing#Overview_.26_Controls) of the fishing minigame. The image below is a screenshot from Stardew Valley's fishing minigame.\n",
    "\n",
    "![The game](https://www.carlsguides.com/stardewvalley/pictures/fishing/catch-fish.jpg)\n",
    "\n",
    "\n",
    "The mini-game is a simple objective of keeping a capture area (aka \"the bobber\") behind a randomly moving capture-point (aka \"the fish\") for long enough to win the game. In the image, the capture-point is the fish and the area you move (the bobber) is the green bar. You can change the velocity of the bobber by accelerating it by either holding space or releasing space and letting gravity pull it down. The capture time left is indicated by the large green bar on the left, the \"capture bar\". The goal is to raise the capture bar to the top of the mini-game window. If the capture bar decreases to the bottom, the fish will escape and you will lose. In the original game each time the minigame starts the fish's difficulty is randomized. However for our purposes the difficulty will be constant.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Because the game was built in Java and our RL algorithm would be implemented in Python, we needed a way to communicate between Python and Java. For this we decided to use socket programming. Further, because of the random movement of the fish, we knew we had to use a neural network to approximate the Q-table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "---\n",
    "For our solution we of course needed a nueral network class. For this, we used some code provided to us in the course, a nueral network class and it's sibling optimizer class. We regularly visited lectures (lectures [17](https://nbviewer.jupyter.org/url/www.cs.colostate.edu/~cs445/notebooks/17%20Reinforcement%20Learning%20with%20Neural%20Network%20as%20Q%20Function.ipynb), [18](https://nbviewer.jupyter.org/url/www.cs.colostate.edu/~cs445/notebooks/18.1%20Reinforcement%20Learning%20to%20Control%20a%20Marble.ipynb)) for information regarding reinforcement learning algorithms, and a bit of code. Everything else was written by us.\n",
    "\n",
    "## Steps taken\n",
    "#### Java-Python Communications\n",
    "Because Landon wrote the game, he implemented the communication between Java and Python. There were several options he could have gone with but he ended up writting a simple java socket server which python would recieve the games state from, and map actions to, all while keeping the game loop synced to the RL algorithm loops.\n",
    "\n",
    "#### Convenient RL Neural Network\n",
    "Keegan integrated the neural network class with a few modifications. He implemented a trivial `EpsilonGreedy` method which had access to the nueral networks `self` variable. He also implemented a template `getReinforcement` method which would be later greatly tuned to increase algorithm performance.\n",
    "\n",
    "#### Main loop\n",
    "Landon worked on the \"main loop\" of our program. The main loop deals with the main processes in the RL algorithm; getting the data for each trial, training the model, saving the results, etc..\n",
    "\n",
    "#### Secondary Code\n",
    "For this project to work properly a lot of secondary code was neccessary. Such functions that were neccesary include a way of easily creating a csv of experiments to be run, and a way of executing experiments. At this point in development, we both contributed to this code.\n",
    "\n",
    "#### Fine tunning\n",
    "To experiment with producing a functional model both Landon and Keegan would frequently adjust what Java reports as the state, and what the reinforcement/reward is defined as.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Main Loop.\n",
    "For our implementation we had two proccesses communicating synchronously. It took considerable effort to make Java work syncronously with Python. In this section we will discuss the two proccess.\n",
    "\n",
    "### The Java Loop:\n",
    "In order to get our stand alone Java game to work with Python Landon needed to make several changes. To begin with, there were several bugs in the game that made the fish inconsitent. To be specific, much of the physics did not have a take into account frame rate. This caused the fish to zoom around (at the speed of sound) at very high framerates. We needed to get fix this so the physics would be consistant at very low framerates. Next, we needed a way of syncing the frame rate up with Python. To do this, we simply added a simple condition check at the begining of the frame loop in Java which would only continue with the rest of the code if Python gave it a singal to do so. After this, we needed a way of easily communicating to Python what the current state of the game was. To do this we simply added a Java method which would grab all the information from the game, and convert it into a string, where it would finally be sent to Python on request. \n",
    "\n",
    "Now that this was complete, we could implement the Java side of the main loop:\n",
    "\n",
    "1. The loop conditional recieves an integer from Python, and if it is not zero the loop body will be executed. Otherwise the loop ends and the game stops. This integer is how Python reports the state of the loops (not to be confused with the state of the game!). This can tell Java to either quit the loop or start a new game (described below).\n",
    "\n",
    "3. Inside the loop, Java uses the same integer (loop-state) as used in the loop conditional to check an if statement. If the condition is true Java resets the game to a random state which is immediatly reported to Python. This code is used in the case of a new Trial to get the initial state.\n",
    "\n",
    "4.  Outside the if statement, Java recieves an integer which will relate to the action Python desires to take. Java will then perform the requested action and process the next frame.\n",
    "\n",
    "5. Next Java will wait untill the last requested frame has fully been rendered/proccessed.\n",
    "\n",
    "6. After the frame has been rendered, Java will send an integer to Python reporting that the frame has successfuly been rendered. This is only required so Python can verify that Java is still running properly.\n",
    "\n",
    "7. Next, Java will grab the current state, convert it to a string, and then send it to Python to proccess. After this step Java return to the loop-conditional.\n",
    "\n",
    "### The Python Loop:\n",
    "The Python code that deals with communications is identical to Java, and as such will be simplified in this section.\n",
    "\n",
    "1. The loop conditional for python checks that the current frame is less than the total amount of frame to be proccessed (`FramePerTrial` * `nTrials`).\n",
    "\n",
    "2. Next, Python reports to Java the state of the loop.\n",
    "\n",
    "3. If the state of the loop is to start a new trial, Python grabs a new state from Java as the initial state and then uses the Epsilon greedy function to generate an initial action.\n",
    "\n",
    "4. Afterward, Python will report the desired action to Java. If Python did not execute the last described step (optional step 3), it will instead use the saved action in a following step.\n",
    "\n",
    "5. Next Python will recieve an int from Java checking it's health, and raise an error if the int is not expected.\n",
    "\n",
    "6. After Python checks the health of Java, Python will recieve the game state from Java in the form of a string. Python will then proccess this String to get a list of integers.\n",
    "\n",
    "7. Given the current state, Python will get the reinforcement and decide the next action to take using epsilon greedy.\n",
    "\n",
    "8. With the new state-action pair, Python will put these values into corresponding array which will later be used for training.\n",
    "\n",
    "9. Python now checks whether or not to end this trial and start a new one (currentFrame % framesPerTrial==0). If a new trial needs to be started, Python will do several things:\n",
    "    1. First Python will proccess the data created over repeated steps of step 8.\n",
    "    2. Next, using the proccessed data, Python will train the neural network with the proccessed data.\n",
    "    3. After training, Python will decay Epsilon to make random movements less likely.\n",
    "    \n",
    "10. If there are more trials to be ran Python will continue looping, otherwise Python will:\n",
    "    1. Tell Java to terminate\n",
    "    2. create several output files and proceed to exit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regards to the loop, we had a significant choice to make: do we reset the game after each trial or not? As evidenced above you can see that we decided to go with reseting after each trial. We went with this approach for a couple of main reasons. The main reason is to expose the algorithm to as many states as possible. We noticed that it took several hundred frames for the game's state to significantly change. To be frank, the majority of this decision was made off of intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reinforcement: A cost.\n",
    "\n",
    "The backbone of reinforcement learning is assining a value to certain state. Some states are more desirable than others, and as such you need a way of telling the Q-table this. In our case, it is quite simple; we want the bobber to be as close to the fish as possible. Furthermore, because the fish moves randomly, we would like the bobbers velocity to match that of the fish. In essence, we are trying to **minimize** the distance between the bobber and the fish, thus our reinforcement is a **cost** we would like to minimize. Considering these simple facts, this is how we came up with our reinforcement:\n",
    "\n",
    "Because the bobber and the fish both have a veloctity, and thus a position, the reinforcement is a little more complex than just how far the bobber is from the fish. Throughout development we kept an ideaology in mind such that there are technically 4 types of states:\n",
    "* `Best Colliding`: The bobber is behind the fish and their velocities are in the same direction (equal normalized velocity).\n",
    "\n",
    "* `Simple Colliding`: The bobber is behind the fish, but they have differing normalized velocities.\n",
    "\n",
    "* `Seperate`: The bobber is not behind the fish, but they have equivalent normalized velocities.\n",
    "\n",
    "* `Poor Seperate`: The bobber is not behind the fish and their normalized velocites are inequivalant.\n",
    "\n",
    "In general we knew we wanted to minimize the reinforcement, and as such we decided that the reinforcement for each state was as such: `Best Colliding` < `Simple Colliding` < `Seperate` < `Poor Seperate`.\n",
    "\n",
    "We first began with a simple reinforcement based off of this ideaoligy that follows the above definition. `Best Colliding` returned 0, `Simple Colliding` returned 1, `Seperate` returned 2, `Poor Seperate` returned 3.\n",
    "\n",
    "After discussing our problem we decided that our reinforcement should also encode the fish-bobber distance. To do this we realized that we couldn't just return the distance between the two. For instance, we considered an example where the fish and bobber are not colliding and moving in the same direction. If the fish's position is greater than the bobber's position, and their velocities are upward, then the subsequent state is `Seperate`, but if their velocites are downward, then the subsequent state is `Poor Seperate`. However, if we just return the distance this ideaology would be lost. Because of this we only encode their distance when the state is `Seperate`. This introduced another problem, `Seperate`'s reinforcement value according to the ideaology should be between 1 and 3, whereas the distance could be anywhere from 0 to 1000. This meant we needed to have a standardized distance between 0 and 1, so we simply take the percentage of the distance over the total possible distance (distance/max_range). This didn't entirly solve our problem though, as the new reinforcement for `Seperate` would be between 0 and 1, when it should be between 1 and 3, so we simply added 1 to the percentage. This gave us a reinforcement function defined as:\n",
    "\n",
    "    if `Best Colliding`:\n",
    "        return 0\n",
    "    elif `Simple Colliding`:\n",
    "        return 1\n",
    "    elif `Seperate`:\n",
    "        return (distance/max_range) + 1\n",
    "    elif `Poor Seperate`:\n",
    "        return 3\n",
    "\n",
    "We will discuss the downside of this strategy in the results Section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For training we made two flexible sccripts; `creatExperiments` and `run`. `createExperiments` simply creates a csv with several experiments which will then be parsed in `run` and executed. `run` as mentioned before parses each experiment, where for every experiment it will launch an instance of the java game, and then run our main reinforcement script `AutoFish` passing in the training arguments. After each experiment `AutoFish` creates a directory with 4 files in it; `ActionState.csv`, `DQN.dump`, `meanReinByTrial.png` and `results.csv`.\n",
    "\n",
    "* `ActionState.csv` is a record of the last-n state-action pairs. This is useful for knowing what action is made in several actions.\n",
    "* `DQN.dump` is the trained Deep-Q-Network (DQN) which was pickled.\n",
    "* `meanReinByTrial.png` is a graph displaying the mean reinforcement per trial. There is also a 20-trial average overlayed in orange. An example of the graph can be found below.\n",
    "* `results.csv` displays the parameters for the experiment (I.E. network architecture, number of frames per trial, etc).\n",
    "\n",
    "We will discuss further in the results exactly what we ran, but over the course of 3 weeks we ran around 100 experiments using these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fish](https://i.imgur.com/S0nNkDT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "----------\n",
    "As explained earlier, over a three or four week period we ran several hundred experiments. For running experiments we had a general philosophy: run whatever and iterate. We wanted to mostly experiment with small changes in code (such as what is reported as state), and as such we had a fixed set of experiments we ran. We barely every iterated on nueral network parameters, we thought that what we had already defined was good enough. Occasionally we would change these parameters to see if they affected our results much but in general most of our testing had little to do with parameters. The majority of the parameters we did end up editing over time were things like the number of frames per trail, and the total amount of trials. For the first couple set of experiments we tested heavily with what was reported in state. For example, our very first experiment set was done with a very simple state. All that was reported was the difference in position (deltaP) and the difference in velocity (deltaV). Here are most of the states we ended up testing (in chronological order):\n",
    "\n",
    "* deltaP, deltaV\n",
    "* bobber-pos, fish-pos, bobber-vel, fish-vel\n",
    "* bobber-pos, fish-pos, normalized-bobber-vel, normalized-fish-vel\n",
    "* normalized-bobber-pos, normalized-fish-pos, normalized-bobber-vel, normalized-fish-vel\n",
    "* bobber-pos, fish-pos, bobber-vel, fish-vel\n",
    "\n",
    "As you can see we ended using the most verbose state we could. We decided that we can always reduce to any of the other formats if we needed to do so. However, as is we are not doing any proccessing of the state (except for where needed to get reinfocement, see ` Reinforcement: A cost` above). Here are a few mean-reinforcement graphs from each state format. You will notice that some of the graphs are critically different. Unfortently we had some other code changes which affected the graphs output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                    state: detlaP, deltaV.\n",
    "![deltaP, deltaV](https://i.imgur.com/0wbi6Wt.png?1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                            state: bobber-pos, fish-pos, bobber-vel, fish-vel.\n",
    "![bobber-pos, fish-pos, bobber-vel, fish-vel](https://i.imgur.com/MiQEYcY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            state: bobber-pos, fish-pos, normalized-bobber-vel, normalized-fish-vel\n",
    "![bobber-pos, fish-pos, normalized-bobber-vel, normalized-fish-vel](https://i.imgur.com/m5fJJV1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    state: normalized-bobber-pos, normalized-fish-pos, normalized-bobber-vel, normalized-fish-vel.\n",
    "![normalized-bobber-pos, normalized-fish-pos, normalized-bobber-vel, normalized-fish-vel.](https://i.imgur.com/b9hFoLp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                    state: bobber-pos, fish-pos, bobber-vel, fish-vel\n",
    "![bobber-pos, fish-pos, bobber-vel, fish-vel](https://i.imgur.com/wAE4RpB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see,  our results were never that great. To capture the fish the reinforcement would be need to be between zero and one (again, see ` Reinforcement: A cost` from above). The only graph that does have substantial reinforcement in our desired range is the very first graph. However, if my memory serves me correctly we had a *slightly* erronous reinforcement function for that method. Further, as you can see, the first three graphs appear to indicate random movement of the bobber (given how some of the blue lines are just a solid block from start to end). This led us to discover a slightly embarrsing bug in our code. We were never decaying epsilon! We totally forgot that we moved Epsilon to be a class variable of the neural network, so the whole time we were decaying *the wrong epsilon*. Once we discovered this and fixed it, we saw much better results which seemed to indicated some level of \"intelligence\" in the bobber's movement. However, as you can plainly see the average reinforcement was *still* unfortunetly not dipping below one like we would hope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several reasons we have considered as to why our algorithm was performing so poorly. The first and most obvious error is human error. It is entirely possible that we have one or more bugs in our code. While we do not think this is the case, it is quite possible that while contributing to the same code something got flipped around and has not yet been discovered. It is (atleast in our experience) pretty hard to garentee the correctness of machine learning code, especially because it is so much easier to just blame bad data.\n",
    "\n",
    "The second possibility we have considered has to do with the game itself, specifically the fish's movement. As mentioned earlier, the fishes movement is random. As is obvious, it is impossible to predict something that is random. You can generalize randomness (as we are attempting to do here), but it is likely that the fishes random movement is just too hard for our algorithm to deal with. If we were to do this again, we would have liked to feed the last n frames into a convolutional nueral network to determine the state. We figure this would allow the model to \"foresee\" when the next change in velocity might be, further generalising the fish's movement. The problem with this approach is that we likely do not have the knoweledge to do something this advanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file ./Kochis-Zweigle.ipynb is 3004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/nbformat/current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nbformat import current\n",
    "import glob\n",
    "name=\"./Kochis-Zweigle\"\n",
    "nbfile = glob.glob(name+'.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
